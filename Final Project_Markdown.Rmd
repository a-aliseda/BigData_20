---
title: "Big Data: Final Project"
author: "Angel Aliseda, Javier Patino, Fernando Regalado, Felipe Salazar"
date: "5/12/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = TRUE,
                      message = TRUE,
                      fig.align = "center")

library(tidyverse)

library(tidytext)
library(SnowballC)
library(tm)

library(topicmodels)

library(rtweet)
library(dplyr)
library(ggplot2)
library(readxl)

set.seed(411234)

theme_set(theme_minimal())
```

## Retrieving the tweets from the accounts of Donald Trump and Joe Biden

## Data transformation
```{r Data}
# Load database from repository
data <- read.csv(file = "Candidates_tweets.csv") %>%
  mutate(text = as.character(text))

var_list <- names(data)
#View(var_list)
dim(data)
```

```{r data one-word-per-row}
# Transformation -> one-word-per-row
data_tokens <- data %>% 
  # tokenizing text from tweets
  unnest_tokens(output = text,                       
                input = text) %>%
  # remove numbers and string to lowercase
  filter(!str_detect(text, "^[0-9]*$")) %>%
  mutate(text = str_to_lower(text)) %>%
  # removing stop words
  anti_join(stop_words, by = c("text" = "word")) %>%
  # stemming words to avoid redundancy
  mutate(word_stem = wordStem(text)) %>%
  # merging with SENTIMENT DICTIONARY
  inner_join(get_sentiments("bing"), by = c("text" = "word"))

dim(data_tokens)
```

```{r data one-tweet-per-row matrix}
# Transformation -> one-tweet-per-row matrix
(data_matrix <- data_tokens %>%
  # get count of each token in each document
  count(X, word_stem) %>%
  # creating a document-term matrix with all features and tf weighting
  cast_dtm(document = X, term = word_stem, value = n) %>%
  # removing tokens that appear in less than 1% of tweets
  removeSparseTerms(sparse = .99))

str(data_matrix)
```

## Topic Modelling -> Fernando
```{r}
topics_trump <- data_tokens %>%
  mutate(screen_name = as.character(screen_name)) %>%
  filter(screen_name == "realDonaldTrump") %>%
  count(X, text) %>%
  cast_dtm(document = X,
           term = text,
           value = n) %>%
  removeSparseTerms(sparse = .95) %>%
  LDA(k = 20, control = list(seed = 411234))

topics_trump <- LDA(data_matrix, k = 20, control = list(seed = 123))
topics_trump

raw.sum=apply(data_matrix,1,FUN=sum)
View(raw.sum)

sum(raw.sum)

data_matrix=data_matrix[raw.sum!=0,]
```

