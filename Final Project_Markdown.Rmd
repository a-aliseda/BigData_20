---
title: "Big Data: Final Project"
author: "Angel Aliseda, Javier Patino, Fernando Regalado, Felipe Salazar"
date: "5/12/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = TRUE,
                      message = TRUE,
                      fig.align = "center")

library(tidyverse)
library(tidytext)
library(SnowballC)
library(tm)
library(topicmodels)
library(rtweet)
library(dplyr)
library(ggplot2)
library(readxl)
library(syuzhet)
library(tibble)
library(wordcloud)
library(gamlr)
library(maptpx)
library(knitr)


set.seed(411234)

theme_set(theme_minimal())
```

## Retrieving the tweets from the accounts of Donald Trump and Joe Biden

## Data transformation
```{r Data}
# Load database from repository
data <- read.csv(file = "Candidates_tweets.csv") %>%
  mutate(text = as.character(text))

var_list <- names(data)
#View(var_list)
dim(data)
```

```{r data one-word-per-row}
# Transformation -> one-word-per-row
data_tokens <- data %>% 
  # tokenizing text from tweets
  unnest_tokens(output = text,                       
                input = text) %>%
  # remove numbers and string to lowercase
  filter(!str_detect(text, "^[0-9]*$")) %>%
  mutate(text = str_to_lower(text)) %>%
  # removing stop words
  anti_join(stop_words, by = c("text" = "word")) %>%
  # stemming words to avoid redundancy
  mutate(word_stem = wordStem(text)) %>%
  # merging with SENTIMENT DICTIONARY
  inner_join(get_sentiments("bing"), by = c("text" = "word"))

dim(data_tokens)
```

```{r data one-tweet-per-row matrix}
# Transformation -> one-tweet-per-row matrix
(data_matrix <- data_tokens %>%
  # get count of each token in each document
  count(X, word_stem) %>%
  # creating a document-term matrix with all features and tf weighting
  cast_dtm(document = X, term = word_stem, value = n))
```

## Topic Modelling

A good start for our exploratory analysis of tweets is trying to find out if there are some recognizable patterns in the way both personalitis write. Thus, we can try to discover and identify some topics under which some words are more likely to be employed than others. This assesment not only allow us to cluster tweets under topics but also we can test if these are useful to predict the impact that tweets could have in social media using measures such the number of favorites or retweets.

```{r data matrix for topics}
# Data matrix for Trump
trump_tm <- data_tokens %>%
  mutate(screen_name = as.character(screen_name)) %>%
  # filtering word "trump"
  filter(screen_name == "realDonaldTrump",
         word_stem != "trump") %>% 
  count(X, word_stem) %>%
  cast_dtm(document = X,
           term = word_stem,
           value = n) %>%
  removeSparseTerms(sparse = .99)

trump_tm <- trump_tm[unique(trump_tm$i),] # removing tweets with no terms remaining

# Data matrix for Biden
biden_tm <- data_tokens %>%
  mutate(screen_name = as.character(screen_name)) %>%
  # filtering word "trump"
  filter(screen_name == "JoeBiden", 
         word_stem != "trump") %>%
  count(X, word_stem) %>%
  cast_dtm(document = X,
           term = word_stem,
           value = n) %>%
  removeSparseTerms(sparse = .99)

biden_tm <- biden_tm[unique(biden_tm$i),] # removing tweets with no terms remaining
```

The analysis for Trump includes `r dim(trump_tm)[1]` tweets containing `r dim(trump_tm)[2]` words after removing stop words and stemming. Likewise, data for Biden includes `r dim(biden_tm)[1]` tweets and `r dim(biden_tm)[2]` words after the same data cleanning. Given this information, we can try different numbers of topics that underlie behind text and decide based on the highest log Bayes Factor (BF). It is important to notice that by clustering for text, we assume that documents (tweets) are drawn from a multinomial mixture in which each word has a different probabilty depending on the topic and each tweet itself is a mixture or combination of these topic. 

### Topic analysis for Trump's tweets:

```{r k selection for Trump}
# Topics for Trump
trump_topics <- topics(trump_tm, K=(2:8), verb=10)
summary(trump_topics, n=10)
```

Taking into account up to 8 different topics for Trump, BF criteria selecst only 6 topics. The following table contains the top ten words with higher probability in each topic:

```{r top ten words Trump}
t1 <- rownames(trump_topics$theta)[order(trump_topics$theta[,1], decreasing=TRUE)[1:10]]
t2 <- rownames(trump_topics$theta)[order(trump_topics$theta[,2], decreasing=TRUE)[1:10]]
t3 <- rownames(trump_topics$theta)[order(trump_topics$theta[,3], decreasing=TRUE)[1:10]]
t4 <- rownames(trump_topics$theta)[order(trump_topics$theta[,4], decreasing=TRUE)[1:10]]
t5 <- rownames(trump_topics$theta)[order(trump_topics$theta[,5], decreasing=TRUE)[1:10]]
t6 <- rownames(trump_topics$theta)[order(trump_topics$theta[,6], decreasing=TRUE)[1:10]]

kable(cbind(t1,t2, t3, t4, t5, t6),
      col.names = c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6"),
      align = "c")
```

Even when these 6 topics contain a variety of words, we can clearly identify that topic 1 and 3 have a positive connotation whereas topic 2, 4, 5 and 6 have are the opposite and related to a more contentious position. This interpretation does not differ much from the president's polarized speech commonly used.

Wordcloud for the more frequent topics in Trump's speech:

```{r wordcloud Trump}
par(mfrow=c(1,2))
wordcloud(row.names(trump_topics$theta),
          random.order = FALSE,
          freq=trump_topics$theta[,1],
          #min.freq=0.0001,
          col="maroon")
wordcloud(row.names(trump_topics$theta),
          random.order = FALSE,
          freq=trump_topics$theta[,2],
          #min.freq=0.0001,
          col="maroon")
```

### Topic analysis for Biden's tweets:

```{r k selection for Biden}
# Topics for Biden
biden_topics <- topics(biden_tm, K=(2:8), verb=10)
summary(biden_topics, n=10)
```

Once again we evalute up to 8 topics for Biden's tweets. BF criteria selects 6 topics again. The following table contains the top ten words with higher probability in each topic:

```{r top ten words Biden}
b1 <- rownames(biden_topics$theta)[order(biden_topics$theta[,1], decreasing=TRUE)[1:10]]
b2 <- rownames(biden_topics$theta)[order(biden_topics$theta[,2], decreasing=TRUE)[1:10]]
b3 <- rownames(biden_topics$theta)[order(biden_topics$theta[,3], decreasing=TRUE)[1:10]]
b4 <- rownames(biden_topics$theta)[order(biden_topics$theta[,4], decreasing=TRUE)[1:10]]
b5 <- rownames(biden_topics$theta)[order(biden_topics$theta[,5], decreasing=TRUE)[1:10]]
b6 <- rownames(biden_topics$theta)[order(biden_topics$theta[,6], decreasing=TRUE)[1:10]]

kable(cbind(b1,b2, b3, b4, b5, b6),
      col.names = c("Topic 1", "Topic 2", "Topic 3", "Topic 4", "Topic 5", "Topic 6"),
      align = "c")
```

In contrast to Trump'tweets, Biden's topics contain less agressive words and the tone of speech seems to be more moderate. This time we don't observe strong words such "hoax", "fraud" or "collusion". Moreover, Biden tends to include more positive words in his tweets since this are more frequent accross topics.

Wordcloud for the more frequent topics in Biden's speech:

```{r wordcloud Biden}
par(mfrow=c(1,2))
wordcloud(row.names(biden_topics$theta),
          random.order = FALSE,
          freq=biden_topics$theta[,1],
          #min.freq=0.0001,
          col="navy")
wordcloud(row.names(biden_topics$theta),
          random.order = FALSE,
          freq=biden_topics$theta[,2],
          #min.freq=0.0001,
          col="navy")
```

### Topic regression

Now we try to find out if clustering is useful for predicting the impact of tweets measured as the number or retweet or times that users shared the information. For this, we relate topics with variable `retweet_count` in a a second-stage low-D regression and compare results with a regression using all words (our original high-D matrix).

### Trump: second-stage low-D regression vs word regression

```{r Topic regression Trump}
# Regress retweets_counts on omega (matrix with tweets topic weights)
trump_rts <- slice(data, as.numeric(trump_tm$dimnames$Docs))
y <- trump_rts[,"retweet_count"]

trump_topics_reg.cv <- cv.gamlr(trump_topics$omega, y,
                        lambda.min.ratio=10^{-4})

coef(trump_topics_reg.cv) # number of retweets up or down for moving up 10\% weight in that topic
```

Results for Trump suggest that only topic 2 have a positive effect on number of retweets. Recall that under this topic, aggressive words are likely to appear such "fake", "enemy", "corrupt" or "crazy". Therefore, results are preliminar evidence that suggest that Trump's confrontational tone is effective at least in Twitter.

```{r Word regression Trump}
## give it the word %s as inputs
trump_words_reg.cv <- cv.gamlr(trump_tm, y)
coef(trump_words_reg.cv)
```

In this sense, regression using words suggests similar findings. Using high-dimensional matrix, word regression finds that the same likely words contained in topic 2 have a significant effect on the number of retweets which adds evidence to signal found in previous regression. Moreover, some additional words are added to the list such "relief", "support" or "crisis" but these have a negative effect on diffusion of tweets.

```{r Comparison reg Trump}
par(mfrow=c(1,2))
plot(trump_topics_reg.cv)
mtext("topic regression", font=2, line=2)
plot(trump_words_reg.cv)
mtext("bigram regression", font=2, line=2)
```

Finally, a comparison of the predictive power of these two regression reveals that topic clustering is less efective than using words. Thus, the later one yields a lower out-of-sample mean squared error which entails a better performance in terms of prediction. 
### Biden: second-stage low-D regression vs word regression

```{r Topic regression Biden}
# Regress retweets_counts on omega (matrix with tweets topic weights)
biden_rts <- slice(data, as.numeric(biden_tm$dimnames$Docs))
y <- biden_rts[,"retweet_count"]

biden_topics_reg.cv <- cv.gamlr(biden_topics$omega, y,
                        lambda.min.ratio=10^{-4})

coef(biden_topics_reg.cv) # number of retweets up or down for moving up 10\% weight in that topic
```

```{r Word regression Biden}
## give it the word %s as inputs
biden_words_reg.cv <- cv.gamlr(biden_tm, y)
coef(biden_words_reg.cv)
```

```{r Comparison reg Biden}
par(mfrow=c(1,2))
plot(biden_topics_reg.cv)
mtext("topic regression", font=2, line=2)
plot(biden_words_reg.cv)
mtext("bigram regression", font=2, line=2)
```

## Sentiment Analysis
```{r, warning=FALSE}
#Sentiment analysis for Trump
data_trump <- data[data$screen_name== "realDonaldTrump",]
data_trump$text_plain <- plain_tweets(data_trump$text)
sa_trump <- get_nrc_sentiment(data_trump$text_plain)
#Transposing the dataset
sa1_trump<-data.frame(t(sa_trump))
#Obtaining count
new_sa_trump <- data.frame(rowSums(sa1_trump))
names(new_sa_trump)[1] <- "count"
new_sa_trump <- cbind("sentiment" = rownames(new_sa_trump), new_sa_trump)
rownames(new_sa_trump) <- NULL


#Sentiment analysis for Biden
data_biden <- data[data$screen_name== "JoeBiden",]
data_biden$text_plain <- plain_tweets(data_biden$text)
sa_biden <- get_nrc_sentiment(data_biden$text_plain)
#Transposing the dataset
sa1_biden<-data.frame(t(sa_biden))
#Obtaining count
new_sa_biden <- data.frame(rowSums(sa1_biden))
names(new_sa_biden)[1] <- "count"
new_sa_biden <- cbind("sentiment" = rownames(new_sa_biden), new_sa_biden)
rownames(new_sa_biden) <- NULL

#Plotting the sentiments
qplot(sentiment, data=new_sa_trump[1:8,], weight=count, geom="bar",fill=sentiment)+ggtitle("Trump's Sentiments")
qplot(sentiment, data=new_sa_biden[1:8,], weight=count, geom="bar",fill=sentiment)+ggtitle("Biden's Sentiments")
qplot(sentiment, data=new_sa_trump[9:10,], weight=count, geom="bar",fill=sentiment)+ggtitle("Trump's Sentiments")
qplot(sentiment, data=new_sa_biden[9:10,], weight=count, geom="bar",fill=sentiment)+ggtitle("Biden's Sentiments")

```

## LASSO analysis for count of words on Favorites and Retweets

```{r fav_rt, warning=FALSE, results='asis'}

### Dependent variables for the LASSO analysis
## Favorites
# Total number of favorites for Trump for tweets included in term matrix
tweets <- as.numeric(data_matrix$dimnames$Docs)

favs_trump <- data %>% 
  select(X, favorite_count) %>% 
  filter(X<3215) %>% 
  filter(X %in% tweets & favorite_count>0) #original tweets included in the term matrix 

# Total number of favorites for Biden for tweets included in term matrix

favs_biden <- data %>% 
  select(X, favorite_count) %>% 
  filter(X>3214) %>% 
  filter(X %in% tweets & favorite_count>0) #original tweets included in the term matrix

## Retweets
# Trump 
rt_trump <- data %>% 
  select(X, retweet_count) %>% 
  filter(X<3215) %>% 
  filter(X %in% tweets) #retweets included in the term matrix

# Biden 
rt_biden <- data %>% 
  select(X, retweet_count) %>% 
  filter(X>3214) %>% 
  filter(X %in% tweets) #retweets included in the term matrix
```

What makes a successful tweet? Are there common words for candidates that gets them more favorites or retweets? In order to answer these questions, we performed a predictive analysis where we looked at whether there are common words that predict if a tweet is succesful. To do this, we used two variables that we obtained from the Twitter database which are total number of Favorites and total number of Retweets. 

We performed a LASSO analysis for both candidates and for Favorites and Retweets sepparately. Trump's tweets have a median of `r median(favs_trump$favorite_count)` favs and a median of `r median(rt_trump$retweet_count)` retweets. On the other hand, Biden's tweets have a median of `r median(favs_biden$favorite_count)` favs and `r median(rt_biden$retweet_count)` retweets. We can see here that, at least in Twitter, candidate Trump is more popular than candidate Biden which is reflection of the amount of time Trump invested in Twitter during his campaign and his presidency. 

The following plots show the LASSO analysis of the total number of favs and retweets by each word using the matrix we computed for the topic analysis. From the models with the lowest AICc, we obtained the top 10 words with the most positive and most negative effect for retweets and favs for each candidate.


``` {r lasso_all, warning=FALSE, results='asis'}
### LASSO for total count of Favorites as outcome variable for each candidate

lasso_favs_trump <- gamlr(data_matrix[favs_trump$X,], 
                          favs_trump$favorite_count)

lasso_favs_biden <- gamlr(data_matrix[as.character(favs_biden$X),],
                          favs_biden$favorite_count)

### LASSO for total count of Retweets as outcome variable for each candidate
lasso_rt_trump <- gamlr(data_matrix[rt_trump$X,], 
                          rt_trump$retweet_count)

lasso_rt_biden <- gamlr(data_matrix[as.character(rt_biden$X),],
                          rt_biden$retweet_count)

### LASSO plots for each model

plot(lasso_favs_trump,
     sub = "LASSO model for words on number of Favorites for Trump")

plot(lasso_favs_biden,
     sub = "LASSO model for words on number of Favorites for Biden")

plot(lasso_rt_trump,
     sub = "LASSO model for words on number of Retweets for Trump")

plot(lasso_rt_biden,
     sub = "LASSO model for words on number of Retweets for Biden")

### Top 5 words with highest effect on Favorites for both candidates
## Top 5 words with most positive effect on favs for Trump

lasso_favs_trump_coefs <- coef(lasso_favs_trump)[-1,]
kable(lasso_favs_trump_coefs[order(-lasso_favs_trump_coefs)][1:5],
      caption = "Trump's top 5 words with most positive effect on favorites")

## Top 5 words with most negative effect on favs for Trump
kable(lasso_favs_trump_coefs[order(lasso_favs_trump_coefs)][1:5],
      caption = "Trump's top 5 words with most negative effect on favorites")

## Top 5 words with most positive effect on favs for Biden
lasso_favs_biden_coefs <- coef(lasso_favs_biden)[-1,]
kable(lasso_favs_biden_coefs[order(-lasso_favs_biden_coefs)][1:5],
      caption = "Biden's top 5 words with most positive effect on favorites")

## Top 5 words with most negative effect on favs for Biden
kable(lasso_favs_biden_coefs[order(lasso_favs_biden_coefs)][1:5],
      caption = "Biden's top 5 words with most negative effect on favorites")

## Top 5 words with most positive effect on rt for Trump
lasso_rt_trump_coefs <- coef(lasso_rt_trump)[-1,]
kable(lasso_rt_trump_coefs[order(-lasso_rt_trump_coefs)][1:5],
      caption = "Trump's top 5 words with most positive effect on retweets")

## Top 5 words with most negative effect on rt for Trump
kable(lasso_rt_trump_coefs[order(lasso_rt_trump_coefs)][1:5],
      caption = "Trump's top 5 words with most negative effect on retweets")

## Top 5 words with most positive effect on rt for Biden
lasso_rt_biden_coefs <- coef(lasso_rt_biden)[-1,]
kable(lasso_rt_biden_coefs[order(-lasso_rt_biden_coefs)][1:5],
      caption = "Biden's top 5 words with most positive effect on retweets")

## Top 5 words with most negative effect on rt for Biden
kable(lasso_rt_biden_coefs[order(lasso_rt_biden_coefs)][1:5],
      caption = "Biden's top 5 words with most negative effect on retweets")


```

In the case of Trump's tweets, we can see that words such as 'obstacle', 'astoundishing', 'uncertain', or 'afraid' are the words with the most positive effects. The largest effect is the word 'bitter', because having that word in a tweet increased `r lasso_favs_trump_coefs[order(-lasso_favs_trump_coefs)][1]` the number of favorites a tweet receives. Moreover, the 10 words that have most negative effects in Trump's tweets are 'collusion', 'fast', 'strong', 'hater', 'approval'. This words relate to the Russia investigation but also they could relate to his continuous tweets about his allegedly high approval rate.
 
On the other hand, Biden's tweets have different words that have a positive effect on the number of favorites. For instance, words like 'incendiary', 'inept', 'unqualified', or 'incompetent' have strong positive effects. These words seem to be related to tweets talking about President Trump and his actions during the administration. Including one of this word in a tweet, is associated with an increase in the number of favorites by up to `r lasso_favs_biden_coefs[order(-lasso_favs_biden_coefs)][1]`. Further, words such as 'dictator' or 'agressive' have the most negative effect on the number of favorites for Biden. Which could signla that, even though Biden's followers like when he criticize President Trump, they do not fall for aggressive comments against him.

The last four tables present the words with the most positive and negative effects on the number of retweets. As we can see for Trump's tweets, the words are similar than the ones we found above. However, for negative effects, we found other words that relate to the impeachment process such as 'subpoena', 'crisis', or 'crime'. This reflects that, even among Trump followers, his tweets about the impeachment process and the Russia Investigation, harmed his popularity.

Moreover, for Biden's tweets a similar pattern arises. Words that describe President Trump as inept or unqualified gave him a wide support among his followers, but aggresive or harsher words against him lowered the popularity of his tweets.


## LASSO analysis for "successful" tweets
```{r lasso_success, warning=FALSE, results='asis'}

### We defined a "successful" tweet if it is on the 90th percentile or more 
success_fav_trump <- favs_trump$favorite_count>quantile(favs_trump$favorite_count,.9)
success_fav_biden <- favs_biden$favorite_count>quantile(favs_biden$favorite_count,.9)

## Analysis for Trump's successful tweets
lasso_favs_trump_success <- gamlr(data_matrix[favs_trump$X,], 
                                  success_fav_trump,
                                  family = "binomial")
plot(lasso_favs_trump_success,
     sub = "LASSO model for words on odds of being a successful tweet for Trump")

lasso_favs_trump_success_coefs <- coef(lasso_favs_trump_success)[-1,]
kable(lasso_favs_trump_success_coefs[order(-lasso_favs_trump_success_coefs)][1:10],
      caption = "Trump's top 10 words with highest effect on odds of being a successful tweet")

## Analysis for Bidens's successful tweets
lasso_favs_biden_success <- gamlr(data_matrix[as.character(favs_biden$X),], 
                                  success_fav_biden,
                                  family = "binomial")
plot(lasso_favs_biden_success,
     sub = "LASSO model for words on odds of being a successful tweet for Biden")

lasso_favs_biden_success_coefs <- coef(lasso_favs_biden_success)[-1,]
kable(lasso_favs_biden_success_coefs[order(-lasso_favs_biden_success_coefs)][1:10],
      caption = "Biden's top 10 words with highest effect on odds of being a successful tweet")

```

Additional to the analysis on the number of favorites and retweets, we performed an analysis on the words that increased the probability of being in a successful tweet. We defined 'successful' as a binary variable that indicates whether a tweet is on the 90th percentile with most favorites for each candidates. We defined this variable this way because of the large difference between the number of favorites each candidate receives.

We used a logistic LASSO with the 'successful' tweet variable as the outcome variable, and all the words we identified above as covariates. For Trump's tweets, the inclusion of words with the root 'horrif', such as 'horrific' or 'horrifying', increases the odds of a tweet to be on the 90th percentil of most successful tweets by `r exp(lasso_favs_trump_success_coefs[order(-lasso_favs_trump_success_coefs)][1])` times, holding everything else constant.

Furthermore, for Biden's tweets words such as 'wound', 'belittle' or 'complacient' are the words that increases the odds for a tweet to be successful. For instance, the word 'wound' increases `r exp(lasso_favs_biden_success_coefs[order(-lasso_favs_biden_success_coefs)][1])`the odds of being a successful tweet ceteris paribus.

In conclusion, from the LASSO model with the lowest AICc we found that there are some specific words that increase or decrease the number of favorites and retweets for each candidate. Moreover, we found that some of those words also are part of the most successful tweets for each candidate. Finally, it is important to notice that even though we found important signals for individual words, the context in which those words were used also matters. Same words could have different meanings depending on the context and this is lost when we look at them sepparately from the whole tweet.

## References

## Session info
```{r}
devtools::session_info()
```